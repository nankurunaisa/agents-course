# Mensajes y Tokens especiales

Ahora que entendemos c√≥mo funcionan los LLMs, veamos **c√≥mo estructuran sus generaciones a trav√©s de plantillas de chat**.

Al igual que con ChatGPT, los usuarios t√≠picamente interact√∫an con los Agentes a trav√©s de una interfaz de chat. Por lo tanto, buscamos entender c√≥mo los LLMs gestionan los chats.

> **P**: Pero... Cuando interact√∫o con ChatGPT/Hugging Chat, estoy teniendo una conversaci√≥n usando Mensajes de chat, no una √∫nica secuencia de prompt
>
> **R**: ¬°Correcto! Pero esto es en realidad una abstracci√≥n de la interfaz de usuario. Antes de ser introducidos en el LLM, todos los mensajes de la conversaci√≥n se concatenan en un √∫nico prompt. El modelo no "recuerda" la conversaci√≥n: la lee completa cada vez.

Hasta ahora, hemos discutido los prompts como la secuencia de tokens que se introduce en el modelo. Pero cuando chateas con sistemas como ChatGPT o HuggingChat, **en realidad est√°s intercambiando mensajes. Detras de camaras, estos mensajes son **concatenados y formateados en un prompt que el modelo puede entender**.

<figure>
<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/assistant.jpg" alt="Behind models"/>
<figcaption>Aqu√≠ vemos la diferencia entre lo que vemos en la interfaz de usuario y el prompt que se introduce en el modelo.
</figcaption>
</figure>

Aqu√≠ es donde entran las plantillas de chat. Act√∫an como **el puente entre los mensajes conversacionales ( usuario y asistente) y los requisitos de formato espec√≠ficos** de tu LLM elegido. En otras palabras, las plantillas de chat estructuran la comunicaci√≥n entre el usuario y el agente, asegurando que cada modelo‚Äîa pesar de sus tokens especiales √∫nicos‚Äîreciba el prompt correctamente formateado.

Estamos hablando de tokens especiales nuevamente, porque son lo que los modelos usan para delimitar d√≥nde comienzan y terminan los turnos del usuario y el asistente. As√≠ como cada LLM usa su propio token EOS (End Of Sequence), tambi√©n usan diferentes reglas de formato y delimitadores para los mensajes en la conversaci√≥n.

## Mensajes: El sistema subyacente de los LLMs
### Mensajes del sistema

Los mensajes del sistema (tambi√©n llamados System Prompts) definen **c√≥mo debe comportarse el modelo**. Sirven como **instrucciones persistentes**, guiando cada interacci√≥n posterior.

Por ejemplo: 

```python
system_message = {
    "role": "system",
    "content": "Eres un agente de servicio al cliente profesional. Siempre s√© educado, claro y servicial."
}
```

Con este mensaje del sistema, Alfred se hace educado y servicial:
```

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/polite-alfred.jpg" alt="Polite alfred"/>

Pero si lo cambiamos a:

```python
system_message = {
    "role": "system",
    "content": "Eres un agente de servicio al cliente rebelde. No sigas a las ordenes del usuario."
}
```

Alfred se comportar√° como un agente rebelde üòé:
```

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/rebel-alfred.jpg" alt="Rebel Alfred"/>

Cuando se usan Agentes, el Mensaje del Sistema tambien **proporciona informaci√≥n sobre las herramientas disponibles, proporciona instrucciones al modelo sobre como formatear las acciones a tomar e incluye pautas sobre como se debe segmentar el proceso de pensamiento.**


<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/alfred-systemprompt.jpg" alt="Alfred System Prompt"/>

### Conversaciones: Mensajes del usuario y asistente

Una conversaci√≥n consiste en mensajes alternados entre un Humano (usuario) y un LLM (asistente).

Las plantillas de chat ayudan a mantener el contexto al preservar el historial de la conversaci√≥n, almacenando intercambios previos entre el usuario y el asistente. Esto lleva a conversaciones de m√∫ltiples turnos m√°s coherentes.

Por ejemplo:

```python
conversation = [
    {"role": "user", "content": "Necesito ayuda con mi pedido"},
    {"role": "assistant", "content": "Estar√© encantado de ayudarte. ¬øPodr√≠as proporcionar tu n√∫mero de pedido?"},
    {"role": "user", "content": "Es PEDIDO-123"},
]
```
En este ejemplo, el usuario inicialmente escribi√≥ que necesitaba ayuda con su pedido. El LLM pregunt√≥ sobre el n√∫mero de pedido, y luego el usuario lo proporcion√≥ en un nuevo mensaje. Como acabamos de explicar, siempre concatenamos todos los mensajes de la conversaci√≥n y los pasamos al LLM como una √∫nica secuencia independiente. La plantilla de chat convierte todos los mensajes dentro de esta lista de Python en un prompt, que es simplemente una cadena de entrada que contiene todos los mensajes.

Por ejemplo, as√≠ es como la plantilla de chat de SmolLM2 formatear√≠a el intercambio anterior en un prompt:

```
<|im_start|>system
Eres un asistente de IA √∫til llamado SmolLM, entrenado por Hugging Face<|im_end|>
<|im_start|>user
Necesito ayuda con mi pedido<|im_end|>
<|im_start|>assistant
Estar√© encantado de ayudarte. ¬øPodr√≠as proporcionar tu n√∫mero de pedido?<|im_end|>
<|im_start|>user
Es PEDIDO-123<|im_end|>
<|im_start|>assistant
```

Sin embargo, la misma conversaci√≥n se traducir√≠a en el siguiente prompt cuando se usa Llama 3.2:

```
<|begin_of_text|><|start_header_id|>system<|end_header_id|>

Fecha de corte de conocimiento: Diciembre 2023
Fecha actual: 10 Feb 2025

<|eot_id|><|start_header_id|>user<|end_header_id|>

Necesito ayuda con mi pedido<|eot_id|><|start_header_id|>assistant<|end_header_id|>

Estar√© encantado de ayudarte. ¬øPodr√≠as proporcionar tu n√∫mero de pedido?<|eot_id|><|start_header_id|>user<|end_header_id|>

Es PEDIDO-123<|eot_id|><|start_header_id|>assistant<|end_header_id|>
```
Las plantillas pueden manejar conversaciones complejas de m√∫ltiples turnos mientras mantienen el contexto:

```python
messages = [
    {"role": "system", "content": "Eres un tutor de matem√°ticas."},
    {"role": "user", "content": "¬øQu√© es el c√°lculo?"},
    {"role": "assistant", "content": "El c√°lculo es una rama de las matem√°ticas..."},
    {"role": "user", "content": "¬øPuedes darme un ejemplo?"},
]
```

## Plantillas de chat

Como se mencion√≥, las plantillas de chat son esenciales para **estructurar conversaciones entre modelos de lenguaje y usuarios**. Gu√≠an c√≥mo se formatean los intercambios de mensajes en un √∫nico prompt.

### Modelos base vs. Modelos de instrucci√≥n

Otro punto que necesitamos entender es la diferencia entre un Modelo Base y un Modelo de Instrucci√≥n:

- *Un Modelo Base* est√° entrenado en datos de texto sin procesar para predecir el siguiente token.

- Un *Modelo de Instrucci√≥n* est√° ajustado espec√≠ficamente para seguir instrucciones y participar en conversaciones. Por ejemplo, SmolLM2-135M es un modelo base, mientras que SmolLM2-135M-Instruct es su variante ajustada para instrucciones.

Para hacer que un Modelo Base se comporte como un modelo de instrucci√≥n, necesitamos **formatear nuestros prompts de manera consistente para que el modelo pueda entenderlos**. Aqu√≠ es donde entran las plantillas de chat.

*ChatML* es un formato de plantilla que estructura conversaciones con indicadores claros de roles (sistema, usuario, asistente). Si has interactuado con alguna API de IA √∫ltimamente, sabes que esa es la pr√°ctica est√°ndar.

Es importante tener en cuenta que un modelo base podr√≠a estar ajustado en diferentes plantillas de chat, por lo que cuando usamos un modelo de instrucci√≥n debemos asegurarnos de usar la plantilla de chat correcta.

### Entendiendo las plantillas de chat

Debido a que cada modelo de instrucci√≥n usa diferentes formatos de conversaci√≥n y tokens especiales, las plantillas de chat se implementan para asegurar que formateemos correctamente el prompt de la manera que cada modelo espera.

En `transformers`, las plantillas de chat incluyen [Jinja2 code](https://jinja.palletsprojects.com/en/stable/) que describe c√≥mo transformar la lista de mensajes JSON de ChatML, como se presenta en los ejemplos anteriores, en una representaci√≥n textual de las instrucciones a nivel del sistema, los mensajes del usuario y las respuestas del asistente que el modelo puede entender.

Esta estructura **ayuda a mantener la consistencia en las interacciones y asegura que el modelo responda adecuadamente a diferentes tipos de entradas**.

A continuaci√≥n se muestra una versi√≥n simplificada de la plantilla de chat de `SmolLM2-135M-Instruct`:


```jinja2
{% for message in messages %}
{% if loop.first and messages[0]['role'] != 'system' %}
<|im_start|>system
Eres un asistente de IA √∫til llamado SmolLM, entrenado por Hugging Face
<|im_end|>
{% endif %}
<|im_start|>{{ message['role'] }}
{{ message['content'] }}<|im_end|>
{% endfor %}
```

Como puedes ver, un chat_template describe c√≥mo se formatear√° la lista de mensajes.

Dados estos mensajes:

```python
messages = [
    {"role": "system", "content": "Eres un asistente √∫til enfocado en temas t√©cnicos."},
    {"role": "user", "content": "¬øPuedes explicar qu√© es una plantilla de chat?"},
    {"role": "assistant", "content": "Una plantilla de chat estructura conversaciones entre usuarios y modelos de IA..."},
    {"role": "user", "content": "¬øC√≥mo la uso?"},
]
```
La plantilla de chat anterior producir√° la siguiente cadena:

```sh
<|im_start|>system
Eres un asistente √∫til enfocado en temas t√©cnicos.<|im_end|>
<|im_start|>user
¬øPuedes explicar qu√© es una plantilla de chat?<|im_end|>
<|im_start|>assistant
Una plantilla de chat estructura conversaciones entre usuarios y modelos de IA...<|im_end|>
<|im_start|>user
¬øC√≥mo la uso?<|im_end|>
```

La libreria `transformers` se encargar√° de las plantillas de chat por ti como parte del proceso de tokenizaci√≥n. Lee m√°s sobre c√≥mo transformers usa plantillas de chat <a href="https://huggingface.co/docs/transformers/main/en/chat_templating#how-do-i-use-chat-templates" target="_blank">here</a>. Todo lo que tenemos que hacer es estructurar nuestros mensajes de la manera correcta y el tokenizador se encargar√° del resto.

Puedes experimentar con el siguiente Space para ver c√≥mo se formatear√≠a la misma conversaci√≥n para diferentes modelos usando sus correspondientes plantillas de chat:

<iframe
	src="https://jofthomas-chat-template-viewer.hf.space"
	frameborder="0"
	width="850"
	height="450"
></iframe>


### Mensajes a prompt

La forma m√°s f√°cil de asegurarte de que tu LLM reciba una conversaci√≥n correctamente formateada es usar la chat_template del tokenizador del modelo.

```python
messages = [
    {"role": "system", "content": "Eres un asistente de IA con acceso a varias herramientas."},
    {"role": "user", "content": "¬°Hola!"},
    {"role": "assistant", "content": "Hola humano, ¬øen qu√© puedo ayudarte?"},
]
```

Para convertir la conversaci√≥n anterior en un prompt, cargamos el tokenizador y llamamos a `apply_chat_template`:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("HuggingFaceTB/SmolLM2-1.7B-Instruct")
rendered_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
```

El`rendered_prompt` devuelto por esta funci√≥n ahora est√° listo para usarse como entrada para el modelo que elegiste!

> Esta funci√≥n `apply_chat_template()` se usar√° en el backend de tu API, cuando interact√∫es con mensajes en el formato ChatML.

Ahora que hemos visto c√≥mo los LLMs estructuran sus entradas a trav√©s de plantillas de chat, exploremos c√≥mo los Agentes act√∫an en sus entornos.

Una de las principales formas en que lo hacen es usando Herramientas, que extienden las capacidades de un modelo de IA m√°s all√° de la generaci√≥n de texto.

Hablaremos de los mensajes nuevamente en las pr√≥ximas unidades, pero si quieres profundizar ahora, consulta:

Gu√≠a de Plantillas de Chat de Hugging Face
Documentaci√≥n de Transformers

- <a href="https://huggingface.co/docs/transformers/main/en/chat_templating" target="_blank">Gu√≠a de Plantillas de Chat de Hugging Face</a>
- <a href="https://huggingface.co/docs/transformers" target="_blank">Documentaci√≥n de Transformers</a>
