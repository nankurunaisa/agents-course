<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/agents-course/blob/main/notebooks/bonus-unit2/monitoring-and-evaluating-agents.ipynb"},
]} />

# Unidad Extra 2: Observabilidad y Evaluaci√≥n de Agentes

<Tip>
Puedes seguir el c√≥digo en <a href="https://huggingface.co/agents-course/notebooks/blob/main/bonus-unit2/monitoring-and-evaluating-agents-notebook.ipynb" target="_blank">este notebook</a> que puedes ejecutar usando Google Colab.
</Tip>

En este notebook, aprenderemos c√≥mo **monitorear los pasos internos (trazos) de nuestro agente de IA** y **evaluar su rendimiento** utilizando herramientas de observabilidad de c√≥digo abierto.

La capacidad de observar y evaluar el comportamiento de un agente es esencial para:
- Depurar problemas cuando las tareas fallan o producen resultados sub√≥ptimos
- Monitorear costos y rendimiento en tiempo real
- Mejorar la fiabilidad y seguridad a trav√©s de retroalimentaci√≥n continua

## Requisitos del Ejercicio üèóÔ∏è

Antes de ejecutar este notebook, aseg√∫rate de que has:

üî≤ üìö  **Estudiado** [Introducci√≥n a los Agentes](https://huggingface.co/learn/agents-course/unit1/introduction)

üî≤ üìö  **Estudiado** [El framework smolagents](https://huggingface.co/learn/agents-course/unit2/smolagents/introduction)

## Paso 0: Instalar las Librer√≠as Necesarias

Necesitaremos algunas librer√≠as que nos permitan ejecutar, monitorear y evaluar nuestros agentes:


```python
%pip install 'smolagents[telemetry]'
%pip install opentelemetry-sdk opentelemetry-exporter-otlp openinference-instrumentation-smolagents
%pip install langfuse datasets 'smolagents[gradio]'
```

## Paso 1: Instrumentar tu Agente

En este notebook, utilizaremos [Langfuse](https://langfuse.com/) como nuestra herramienta de observabilidad, pero puedes usar **cualquier otro servicio compatible con OpenTelemetry**. El c√≥digo a continuaci√≥n muestra c√≥mo configurar variables de entorno para Langfuse (o cualquier endpoint OTel) y c√≥mo instrumentar tu smolagent.

**Nota:** Si est√°s utilizando LlamaIndex o LangGraph, puedes encontrar documentaci√≥n sobre c√≥mo instrumentarlos [aqu√≠](https://langfuse.com/docs/integrations/llama-index/workflows) y [aqu√≠](https://langfuse.com/docs/integrations/langchain/example-python-langgraph). 

Primero, vamos a configurar la variable de entorno correcta para establecer la conexi√≥n con el endpoint OpenTelemetry de Langfuse. 

```python
import os
import base64

# Obt√©n tus propias claves desde https://cloud.langfuse.com
LANGFUSE_PUBLIC_KEY = = "pk-lf-..." 
LANGFUSE_SECRET_KEY = "sk-lf-..." 
os.environ["LANGFUSE_PUBLIC_KEY"] = LANGFUSE_PUBLIC_KEY
os.environ["LANGFUSE_SECRET_KEY"] = LANGFUSE_SECRET_KEY
os.environ["LANGFUSE_HOST"] = "https://cloud.langfuse.com"  # üá™üá∫ ejemplo de regi√≥n EU
# os.environ["LANGFUSE_HOST"] = "https://us.cloud.langfuse.com"  # üá∫üá∏ ejemplo de regi√≥n US

LANGFUSE_AUTH = base64.b64encode(
    f"{LANGFUSE_PUBLIC_KEY}:{LANGFUSE_SECRET_KEY}".encode()
).decode()

os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"] = os.environ.get("LANGFUSE_HOST") + "/api/public/otel"
os.environ["OTEL_EXPORTER_OTLP_HEADERS"] = f"Authorization=Basic {LANGFUSE_AUTH}"
```
Tambi√©n necesitamos configurar nuestro token de Hugging Face para las llamadas de inferencia.

```python
# Configura tu token de Hugging Face y otros tokens/secretos como variables de entorno
os.environ["HF_TOKEN"] = "hf_..." 
```
A continuaci√≥n, podemos configurar un proveedor de trazoss para nuestro OpenTelemetry configurado.

```python
from opentelemetry.sdk.trace import TracerProvider
from openinference.instrumentation.smolagents import SmolagentsInstrumentor
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.trace.export import SimpleSpanProcessor
 
# Crear un TracerProvider para OpenTelemetry
trace_provider = TracerProvider()

# A√±adir un SimpleSpanProcessor con el OTLPSpanExporter para enviar trazoss
trace_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter()))

# Establecer el proveedor de trazas predeterminado global
from opentelemetry import trace
trace.set_tracer_provider(trace_provider)
tracer = trace.get_tracer(__name__)

# Instrumentar smolagents con el proveedor configurado
SmolagentsInstrumentor().instrument(tracer_provider=trace_provider)
```

## Paso 2: Probar tu Instrumentaci√≥n

Aqu√≠ hay un simple CodeAgent de smolagents que calcula `1+1`. Lo ejecutamos para confirmar que la instrumentaci√≥n est√° funcionando correctamente. Si todo est√° configurado correctamente, ver√°s registros/spans en tu panel de observabilidad.


```python
from smolagents import InferenceClientModel, CodeAgent

# Crear un agente simple para probar la instrumentaci√≥n
agent = CodeAgent(
    tools=[],
    model=InferenceClientModel()
)

agent.run("1+1=")
```

Revisa tu [Panel de rastros de Langfuse](https://cloud.langfuse.com/traces) (o tu herramienta de observabilidad elegida) para confirmar que los spans y registros han sido grabados.

Captura de pantalla de ejemplo de Langfuse:

![Ejemplo de rastros en Langfuse](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/first-example-trace.png)

_[Enlace a  los rastros](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/1b94d6888258e0998329cdb72a371155?timestamp=2025-03-10T11%3A59%3A41.743Z)_

## Paso 3: Observar y Evaluar un Agente M√°s Complejo

Ahora que has confirmado que tu instrumentaci√≥n funciona, probemos una consulta m√°s compleja para ver c√≥mo se rastrean las m√©tricas avanzadas (uso de tokens, latencia, costos, etc.).


```python
from smolagents import (CodeAgent, DuckDuckGoSearchTool, InferenceClientModel)

search_tool = DuckDuckGoSearchTool()
agent = CodeAgent(tools=[search_tool], model=InferenceClientModel())

agent.run("¬øCu√°ntos cubos de Rubik podr√≠as meter dentro de la Catedral de Notre Dame?")
```

### Estructura de Rastros

La mayor√≠a de las herramientas de observabilidad registran una **rastro** que contiene **spans**, que representan cada paso de la l√≥gica de tu agente. Aqu√≠, el rastro contiene la ejecuci√≥n general del agente y sub-spans para:
- Las llamadas a herramientas (DuckDuckGoSearchTool)
- Las llamadas al LLM (InferenceClientModel)

Puedes inspeccionarlos para ver precisamente d√≥nde se gasta el tiempo, cu√°ntos tokens se utilizan, etc.:

![√Årbol de rastros en Langfuse](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/trace-tree.png)

_[Enlace a los rastros](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/1ac33b89ffd5e75d4265b62900c348ed?timestamp=2025-03-07T13%3A45%3A09.149Z&display=preview)_

## Evaluaci√≥n en L√≠nea

En la secci√≥n anterior, aprendimos sobre la diferencia entre evaluaci√≥n en l√≠nea y fuera de l√≠nea. Ahora, veremos c√≥mo monitorear tu agente en producci√≥n y evaluarlo en vivo.

### M√©tricas Comunes para Seguir en Producci√≥n

1. **Costos** ‚Äî La instrumentaci√≥n de smolagents captura el uso de tokens, que puedes transformar en costos aproximados asignando un precio por token.
2. **Latencia** ‚Äî Observa el tiempo que toma completar cada paso, o la ejecuci√≥n completa.
3. **Retroalimentaci√≥n del Usuario** ‚Äî Los usuarios pueden proporcionar retroalimentaci√≥n directa (pulgar arriba/abajo) para ayudar a refinar o corregir el agente.
4. **LLM-como-Juez** ‚Äî Utiliza un LLM separado para evaluar la salida de tu agente en tiempo casi real (por ejemplo, verificando toxicidad o correcci√≥n).

A continuaci√≥n, mostramos ejemplos de estas m√©tricas.

#### 1. Costos

A continuaci√≥n se muestra una captura de pantalla que muestra el uso para llamadas a `Qwen2.5-Coder-32B-Instruct`. Esto es √∫til para ver pasos costosos y optimizar tu agente. 

![Costos](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/smolagents-costs.png)

_[Enlace a los rastros](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/1ac33b89ffd5e75d4265b62900c348ed?timestamp=2025-03-07T13%3A45%3A09.149Z&display=preview)_

#### 2. Latencia

Tambi√©n podemos ver cu√°nto tiempo tom√≥ completar cada paso. En el ejemplo a continuaci√≥n, toda la conversaci√≥n tom√≥ 32 segundos, que puedes desglosar por paso. Esto te ayuda a identificar cuellos de botella y optimizar tu agente.

![Latencia](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/smolagents-latency.png)

_[Enlace a los rastros](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/1ac33b89ffd5e75d4265b62900c348ed?timestamp=2025-03-07T13%3A45%3A09.149Z&display=preview)_

#### 3. Atributos Adicionales

Tambi√©n puedes pasar atributos adicionales, como IDs de usuario, IDs de sesi√≥n o etiquetas, configur√°ndolos en los spans. Por ejemplo, la instrumentaci√≥n de smolagents utiliza OpenTelemetry para adjuntar atributos como `langfuse.user.id` o etiquetas personalizadas.


```python
from smolagents import (CodeAgent, DuckDuckGoSearchTool, InferenceClientModel)
from opentelemetry import trace

search_tool = DuckDuckGoSearchTool()
agent = CodeAgent(
    tools=[search_tool],
    model=InferenceClientModel()
)

with tracer.start_as_current_span("Smolagent-Trace") as span:
    span.set_attribute("langfuse.user.id", "smolagent-user-123")
    span.set_attribute("langfuse.session.id", "smolagent-session-123456789")
    span.set_attribute("langfuse.tags", ["city-question", "testing-agents"])

    agent.run("¬øCu√°l es la capital de Alemania?")
```

![Mejorando las ejecuciones de agentes con m√©tricas adicionales](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/smolagents-attributes.png)

#### 4. Retroalimentaci√≥n del Usuario

Si tu agente est√° integrado en una interfaz de usuario, puedes registrar la retroalimentaci√≥n directa del usuario (como un pulgar arriba/abajo en una interfaz de chat). A continuaci√≥n se muestra un ejemplo utilizando [Gradio](https://gradio.app/) para integrar un chat con un mecanismo de retroalimentaci√≥n simple.

En el fragmento de c√≥digo a continuaci√≥n, cuando un usuario env√≠a un mensaje de chat, capturamos el ID de traza de OpenTelemetry. Si al usuario le gusta/no le gusta la √∫ltima respuesta, adjuntamos una puntuaci√≥n a la traza.


```python
import gradio as gr
from opentelemetry.trace import format_trace_id
from smolagents import (CodeAgent, InferenceClientModel)
from langfuse import Langfuse

langfuse = Langfuse()
model = InferenceClientModel()
agent = CodeAgent(tools=[], model=model, add_base_tools=True)

formatted_trace_id = None  # Almacenaremos el trace_id actual globalmente para demostraci√≥n

def respond(prompt, history):
    with trace.get_tracer(__name__).start_as_current_span("Smolagent-Trace") as span:
        output = agent.run(prompt)

        current_span = trace.get_current_span()
        span_context = current_span.get_span_context()
        trace_id = span_context.trace_id
        global formatted_trace_id
        formatted_trace_id = str(format_trace_id(trace_id))
        langfuse.trace(id=formatted_trace_id, input=prompt, output=output)

    history.append({"role": "assistant", "content": str(output)})
    return history

def handle_like(data: gr.LikeData):
    # Para demostraci√≥n, mapeamos la retroalimentaci√≥n del usuario a un 1 (me gusta) o 0 (no me gusta)
    if data.liked:
        langfuse.score(
            value=1,
            name="user-feedback",
            trace_id=formatted_trace_id
        )
    else:
        langfuse.score(
            value=0,
            name="user-feedback",
            trace_id=formatted_trace_id
        )

with gr.Blocks() as demo:
    chatbot = gr.Chatbot(label="Chat", type="messages")
    prompt_box = gr.Textbox(placeholder="Escribe tu mensaje...", label="Tu mensaje")

    # Cuando el usuario presiona 'Enter' en el prompt, ejecutamos 'respond'
    prompt_box.submit(
        fn=respond,
        inputs=[prompt_box, chatbot],
        outputs=chatbot
    )

    # Cuando el usuario hace clic en un bot√≥n de 'me gusta' en un mensaje, ejecutamos 'handle_like'
    chatbot.like(handle_like, None, None)

demo.launch()

```

La retroalimentaci√≥n del usuario se captura entonces en tu herramienta de observabilidad:

![La retroalimentaci√≥n del usuario se captura en Langfuse](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/user-feedback-gradio.png)

#### 5. LLM-como-Juez

LLM-como-Juez es otra forma de evaluar autom√°ticamente la salida de tu agente. Puedes configurar una llamada LLM separada para medir la correcci√≥n, toxicidad, estilo o cualquier otro criterio que te importe.

**Flujo de trabajo**:
1. Defines una **Plantilla de Evaluaci√≥n**, por ejemplo, "Verifica si el texto es t√≥xico".
2. Cada vez que tu agente genera una salida, pasas esa salida a tu LLM "juez" con la plantilla.
3. El LLM juez responde con una calificaci√≥n o etiqueta que registras en tu herramienta de observabilidad.

Ejemplo de Langfuse:

![Plantilla de Evaluaci√≥n LLM-como-Juez](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/evaluator-template.png)
![Evaluador LLM-como-Juez](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/evaluator.png)


```python
# Ejemplo: Verificar si la salida del agente es t√≥xica o no.
from smolagents import (CodeAgent, DuckDuckGoSearchTool, InferenceClientModel)

search_tool = DuckDuckGoSearchTool()
agent = CodeAgent(tools=[search_tool], model=InferenceClientModel())

agent.run("¬øPuede comer zanahorias mejorar tu visi√≥n?")
```

Puedes ver que la respuesta de este ejemplo se juzga como "no t√≥xica".

![Puntuaci√≥n de Evaluaci√≥n LLM-como-Juez](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/llm-as-a-judge-score.png)

#### 6. Resumen de M√©tricas de Observabilidad

Todas estas m√©tricas pueden visualizarse juntas en paneles. Esto te permite ver r√°pidamente c√≥mo se desempe√±a tu agente a trav√©s de muchas sesiones y te ayuda a seguir las m√©tricas de calidad a lo largo del tiempo.

![Resumen de m√©tricas de observabilidad](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/langfuse-dashboard.png)

## Evaluaci√≥n fuera de l√≠nea

La evaluaci√≥n en l√≠nea es esencial para la retroalimentaci√≥n en vivo, pero tambi√©n necesitas **evaluaci√≥n fuera de l√≠nea**‚Äîverificaciones sistem√°ticas antes o durante el desarrollo. Esto ayuda a mantener la calidad y fiabilidad antes de implementar cambios en producci√≥n.

### Evaluaci√≥n con Conjuntos de Datos

En la evaluaci√≥n fuera de l√≠nea, t√≠picamente:
1. Tienes un conjunto de datos de referencia (con pares de prompt y salida esperada)
2. Ejecutas tu agente en ese conjunto de datos
3. Comparas las salidas con los resultados esperados o utilizas un mecanismo de puntuaci√≥n adicional

A continuaci√≥n, demostramos este enfoque con el [conjunto de datos GSM8K](https://huggingface.co/datasets/gsm8k), que contiene preguntas matem√°ticas y soluciones.


```python
import pandas as pd
from datasets import load_dataset

# Obtener GSM8K desde Hugging Face
dataset = load_dataset("openai/gsm8k", 'main', split='train')
df = pd.DataFrame(dataset)
print("Primeras filas del conjunto de datos GSM8K:")
print(df.head())
```

A continuaci√≥n, creamos una entidad de conjunto de datos en Langfuse para rastrear las ejecuciones. Luego, agregamos cada elemento del conjunto de datos al sistema. (Si no est√°s utilizando Langfuse, podr√≠as simplemente almacenar estos en tu propia base de datos o archivo local para an√°lisis).


```python
from langfuse import Langfuse
langfuse = Langfuse()

langfuse_dataset_name = "gsm8k_dataset_huggingface"

# Crear un conjunto de datos en Langfuse
langfuse.create_dataset(
    name=langfuse_dataset_name,
    description="Conjunto de datos de referencia GSM8K cargado desde Huggingface",
    metadata={
        "date": "2025-03-10", 
        "type": "benchmark"
    }
)
```


```python
for idx, row in df.iterrows():
    langfuse.create_dataset_item(
        dataset_name=langfuse_dataset_name,
        input={"text": row["question"]},
        expected_output={"text": row["answer"]},
        metadata={"source_index": idx}
    )
    if idx >= 9: # Cargar solo los primeros 10 elementos para demostraci√≥n
        break
```

![Elementos del conjunto de datos en Langfuse](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/example-dataset.png)

#### Ejecutando el Agente en el Conjunto de Datos

Definimos una funci√≥n auxiliar `run_smolagent()` que:
1. Inicia un span de OpenTelemetry
2. Ejecuta nuestro agente en el prompt
3. Registra el ID de traza en Langfuse

Luego, recorremos cada elemento del conjunto de datos, ejecutamos el agente y vinculamos el rastro al elemento del conjunto de datos. Tambi√©n podemos adjuntar una puntuaci√≥n de evaluaci√≥n r√°pida si lo deseamos.


```python
from opentelemetry.trace import format_trace_id
from smolagents import (CodeAgent, InferenceClientModel, LiteLLMModel)

# Ejemplo: usando InferenceClientModel o LiteLLMModel para acceder a modelos de openai, anthropic, gemini, etc.:
model = InferenceClientModel()

agent = CodeAgent(
    tools=[],
    model=model,
    add_base_tools=True
)

def run_smolagent(question):
    with tracer.start_as_current_span("Smolagent-Trace") as span:
        span.set_attribute("langfuse.tag", "dataset-run")
        output = agent.run(question)

        current_span = trace.get_current_span()
        span_context = current_span.get_span_context()
        trace_id = span_context.trace_id
        formatted_trace_id = format_trace_id(trace_id)

        langfuse_trace = langfuse.trace(
            id=formatted_trace_id, 
            input=question, 
            output=output
        )
    return langfuse_trace, output
```


```python
dataset = langfuse.get_dataset(langfuse_dataset_name)

# Ejecutar nuestro agente contra cada elemento del conjunto de datos (limitado a los primeros 10 arriba)
for item in dataset.items:
    langfuse_trace, output = run_smolagent(item.input["text"])

    # Vincular la traza al elemento del conjunto de datos para an√°lisis
    item.link(
        langfuse_trace,
        run_name="smolagent-notebook-run-01",
        run_metadata={ "model": model.model_id }
    )

    # Opcionalmente, almacenar una puntuaci√≥n de evaluaci√≥n r√°pida para demostraci√≥n
    langfuse_trace.score(
        name="<example_eval>",
        value=1,
        comment="Este es un comentario"
    )

# Vaciar datos para asegurar que toda la telemetr√≠a sea enviada
langfuse.flush()
```

Puedes repetir este proceso con diferentes:
- Modelos (OpenAI GPT, LLM local, etc.)
- Herramientas (b√∫squeda vs. sin b√∫squeda)
- Prompts (diferentes mensajes de sistema)

Luego compararlos lado a lado en tu herramienta de observabilidad:

![Resumen de ejecuci√≥n del conjunto de datos](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/dataset_runs.png)
![Comparaci√≥n de ejecuci√≥n del conjunto de datos](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/dataset-run-comparison.png)


## Consideraciones Finales

En este notebook, cubrimos c√≥mo:
1. **Configurar la Observabilidad** usando smolagents + exportadores OpenTelemetry
2. **Verificar la Instrumentaci√≥n** ejecutando un agente simple
3. **Capturar M√©tricas Detalladas** (costo, latencia, etc.) a trav√©s de herramientas de observabilidad
4. **Recopilar Retroalimentaci√≥n del Usuario** a trav√©s de una interfaz Gradio
5. **Usar LLM-como-Juez** para evaluar autom√°ticamente las salidas
6. **Realizar Evaluaci√≥n Fuera de L√≠nea** con un conjunto de datos de referencia

ü§ó ¬°Feliz programaci√≥n!
