# Observabilidad y Evaluaci√≥n de Agentes de IA

## üîé ¬øQu√© es la Observabilidad?

La observabilidad consiste en entender qu√© est√° sucediendo dentro de tu agente de IA mediante el an√°lisis de se√±ales externas como registros, m√©tricas y rastros. Para los agentes de IA, esto significa rastrear acciones, uso de herramientas, llamadas al modelo y respuestas para depurar y mejorar el rendimiento del agente.

![Panel de observabilidad](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/langfuse-dashboard.png)

## üî≠ ¬øPor qu√© es Importante la Observabilidad de Agentes?

Sin observabilidad, los agentes de IA son "cajas negras". Las herramientas de observabilidad hacen que los agentes sean transparentes, permiti√©ndote:

- Entender el intercambio entre costos y precisi√≥n
- Medir la latencia
- Detectar lenguaje da√±ino e inyecci√≥n de prompts
- Monitorear la retroalimentaci√≥n del usuario

En otras palabras, ¬°hace que tu agente de demostraci√≥n est√© listo para producci√≥n!

## üî® Herramientas de Observabilidad

Las herramientas comunes de observabilidad para agentes de IA incluyen plataformas como [Langfuse](https://langfuse.com) y [Arize](https://www.arize.com). Estas herramientas ayudan a recopilar rastros detallados y ofrecen paneles para monitorear m√©tricas en tiempo real, facilitando la detecci√≥n de problemas y la optimizaci√≥n del rendimiento.

Las herramientas de observabilidad var√≠an ampliamente en sus caracter√≠sticas y capacidades. Algunas herramientas son de c√≥digo abierto, benefici√°ndose de grandes comunidades que dan forma a sus hojas de ruta y extensas integraciones. Adem√°s, ciertas herramientas se especializan en aspectos espec√≠ficos de LLMOps, como observabilidad, evaluaciones o gesti√≥n de prompts, mientras que otras est√°n dise√±adas para cubrir todo el flujo de trabajo de LLMOps. Te animamos a explorar la documentaci√≥n de diferentes opciones para elegir una soluci√≥n que funcione bien para ti.

Muchos frameworks de agentes como [smolagents](https://smolagents.com) utilizan el est√°ndar [OpenTelemetry](https://opentelemetry.io/docs/) para exponer metadatos a las herramientas de observabilidad. Adem√°s de esto, las herramientas de observabilidad construyen instrumentaciones personalizadas para permitir m√°s flexibilidad en el mundo r√°pidamente cambiante de los LLM. Debes consultar la documentaci√≥n de la herramienta que est√°s utilizando para ver qu√© es compatible.

## üî¨ Rastros y Spans

Las herramientas de observabilidad generalmente representan las ejecuciones de agentes como rastros y spans.

- Los **Rastros** representan una tarea completa del agente de principio a fin (como manejar una consulta de usuario).
- Los **Spans** son pasos individuales dentro del rastro (como llamar a un modelo de lenguaje o recuperar datos).

![Ejemplo de un rastro de smolagent en Langfuse](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/trace-tree.png)

## üìä M√©tricas Clave para Monitorear

Aqu√≠ hay algunas de las m√©tricas m√°s comunes que las herramientas de observabilidad monitorean:

**Latencia:** ¬øCon qu√© rapidez responde el agente? Los tiempos de espera largos afectan negativamente la experiencia del usuario. Debes medir la latencia para tareas y pasos individuales rastreando las ejecuciones del agente. Por ejemplo, un agente que tarda 20 segundos para todas las llamadas al modelo podr√≠a acelerarse utilizando un modelo m√°s r√°pido o ejecutando llamadas al modelo en paralelo.

**Costos:** ¬øCu√°l es el gasto por ejecuci√≥n del agente? Los agentes de IA dependen de llamadas a LLM facturadas por token o APIs externas. El uso frecuente de herramientas o m√∫ltiples prompts puede aumentar r√°pidamente los costos. Por ejemplo, si un agente llama a un LLM cinco veces para una mejora marginal de calidad, debes evaluar si el costo est√° justificado o si podr√≠as reducir el n√∫mero de llamadas o usar un modelo m√°s econ√≥mico. El monitoreo en tiempo real tambi√©n puede ayudar a identificar picos inesperados (por ejemplo, errores que causan bucles excesivos de API).

**Errores de Solicitud:** ¬øCu√°ntas solicitudes fall√≥ el agente? Esto puede incluir errores de API o llamadas fallidas a herramientas. Para hacer que tu agente sea m√°s robusto contra estos en producci√≥n, puedes configurar alternativas o reintentos. Por ejemplo, si el proveedor de LLM A est√° ca√≠do, cambias al proveedor de LLM B como respaldo.

**Retroalimentaci√≥n del Usuario:** Implementar evaluaciones directas del usuario proporciona informaci√≥n valiosa. Esto puede incluir calificaciones expl√≠citas (pulgar arriba üëç/abajo üëé, 1-5 estrellas ‚≠ê) o comentarios textuales. La retroalimentaci√≥n negativa consistente debe alertarte, ya que es una se√±al de que el agente no est√° funcionando como se esperaba.

**Retroalimentaci√≥n Impl√≠cita del Usuario:** Los comportamientos del usuario proporcionan retroalimentaci√≥n indirecta incluso sin calificaciones expl√≠citas. Esto puede incluir reformulaci√≥n inmediata de preguntas, consultas repetidas o hacer clic en un bot√≥n de reintento. Por ejemplo, si ves que los usuarios hacen repetidamente la misma pregunta, esto es una se√±al de que el agente no est√° funcionando como se esperaba.

**Precisi√≥n:** ¬øCon qu√© frecuencia produce el agente resultados correctos o deseables? Las definiciones de precisi√≥n var√≠an (por ejemplo, correcci√≥n en la resoluci√≥n de problemas, precisi√≥n en la recuperaci√≥n de informaci√≥n, satisfacci√≥n del usuario). El primer paso es definir c√≥mo se ve el √©xito para tu agente. Puedes rastrear la precisi√≥n mediante verificaciones automatizadas, puntuaciones de evaluaci√≥n o etiquetas de finalizaci√≥n de tareas. Por ejemplo, marcar rastros como "exitosos" o "fallidos".

**M√©tricas de Evaluaci√≥n Automatizadas:** Tambi√©n puedes configurar evaluaciones automatizadas. Por ejemplo, puedes usar un LLM para puntuar la salida del agente, por ejemplo, si es √∫til, precisa o no. Tambi√©n hay varias bibliotecas de c√≥digo abierto que te ayudan a puntuar diferentes aspectos del agente. Por ejemplo, [RAGAS](https://docs.ragas.io/) para agentes RAG o [LLM Guard](https://llm-guard.com/) para detectar lenguaje da√±ino o inyecci√≥n de prompts.

En la pr√°ctica, una combinaci√≥n de estas m√©tricas proporciona la mejor cobertura de la salud de un agente de IA. En el [notebook de ejemplo](notebooks/bonus-unit2/monitoring-and-evaluating-agents.ipynb) de este cap√≠tulo, te mostraremos c√≥mo se ven estas m√©tricas en ejemplos reales, pero primero, aprenderemos c√≥mo es un flujo de trabajo de evaluaci√≥n t√≠pico.

## üëç Evaluando Agentes de IA

La observabilidad nos proporciona m√©tricas, pero la evaluaci√≥n es el proceso de analizar esos datos (y realizar pruebas) para determinar qu√© tan bien est√° funcionando un agente de IA y c√≥mo se puede mejorar. En otras palabras, una vez que tienes esos rastros y m√©tricas, ¬øc√≥mo los utilizas para juzgar al agente y tomar decisiones?

La evaluaci√≥n regular es importante porque los agentes de IA a menudo son no deterministas y pueden evolucionar (a trav√©s de actualizaciones o comportamiento cambiante del modelo) - sin evaluaci√≥n, no sabr√≠as si tu "agente inteligente" est√° realmente haciendo bien su trabajo o si ha retrocedido.

Hay dos categor√≠as de evaluaciones para agentes de IA: **evaluaci√≥n en l√≠nea** y **evaluaci√≥n fuera de l√≠nea**. Ambas son valiosas y se complementan entre s√≠. Generalmente comenzamos con la evaluaci√≥n fuera de l√≠nea, ya que este es el paso m√≠nimo necesario antes de implementar cualquier agente.

### ü•∑ Evaluaci√≥n Fuera de L√≠nea

![Elementos del conjunto de datos en Langfuse](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/example-dataset.png)

Esto implica evaluar al agente en un entorno controlado, t√≠picamente utilizando conjuntos de datos de prueba, no consultas de usuarios en vivo. Utilizas conjuntos de datos curados donde sabes cu√°l es la salida esperada o el comportamiento correcto, y luego ejecutas tu agente en ellos.

Por ejemplo, si construiste un agente de problemas de palabras matem√°ticas, podr√≠as tener un [conjunto de datos de prueba](https://huggingface.co/datasets/gsm8k) de 100 problemas con respuestas conocidas. La evaluaci√≥n fuera de l√≠nea a menudo se realiza durante el desarrollo (y puede ser parte de los pipelines de CI/CD) para verificar mejoras o proteger contra regresiones. El beneficio es que es **repetible y puedes obtener m√©tricas claras de precisi√≥n ya que tienes la verdad fundamental**. Tambi√©n podr√≠as simular consultas de usuarios y medir las respuestas del agente contra respuestas ideales o usar m√©tricas automatizadas como se describi√≥ anteriormente.

El desaf√≠o clave con la evaluaci√≥n fuera de l√≠nea es asegurar que tu conjunto de datos de prueba sea completo y se mantenga relevante - el agente podr√≠a funcionar bien en un conjunto de prueba fijo pero encontrar consultas muy diferentes en producci√≥n. Por lo tanto, debes mantener los conjuntos de prueba actualizados con nuevos casos extremos y ejemplos que reflejen escenarios del mundo real. Una mezcla de peque√±os casos de "prueba de humo" y conjuntos de evaluaci√≥n m√°s grandes es √∫til: conjuntos peque√±os para verificaciones r√°pidas y m√°s grandes para m√©tricas de rendimiento m√°s amplias.

### üîÑ Evaluaci√≥n en L√≠nea

Esto se refiere a evaluar al agente en un entorno en vivo y del mundo real, es decir, durante el uso real en producci√≥n. La evaluaci√≥n en l√≠nea implica monitorear el rendimiento del agente en interacciones reales con usuarios y analizar los resultados continuamente.

Por ejemplo, podr√≠as rastrear tasas de √©xito, puntuaciones de satisfacci√≥n del usuario u otras m√©tricas en tr√°fico en vivo. La ventaja de la evaluaci√≥n en l√≠nea es que **captura cosas que podr√≠as no anticipar en un entorno de laboratorio** - puedes observar la deriva del modelo con el tiempo (si la efectividad del agente se degrada a medida que cambian los patrones de entrada) y detectar consultas o situaciones inesperadas que no estaban en tus datos de prueba. Proporciona una imagen verdadera de c√≥mo se comporta el agente en el mundo real.

La evaluaci√≥n en l√≠nea a menudo implica recopilar retroalimentaci√≥n impl√≠cita y expl√≠cita del usuario, como se discuti√≥, y posiblemente ejecutar pruebas sombra o pruebas A/B (donde una nueva versi√≥n del agente se ejecuta en paralelo para comparar con la antigua). El desaf√≠o es que puede ser complicado obtener etiquetas o puntuaciones confiables para interacciones en vivo - podr√≠as depender de la retroalimentaci√≥n del usuario o m√©tricas posteriores (como si el usuario hizo clic en el resultado).

### ü§ù Combinando ambas

En la pr√°ctica, la evaluaci√≥n exitosa de agentes de IA combina m√©todos **en l√≠nea** y **fuera de l√≠nea**. Podr√≠as ejecutar puntos de referencia fuera de l√≠nea regulares para puntuar cuantitativamente a tu agente en tareas definidas y monitorear continuamente el uso en vivo para detectar cosas que los puntos de referencia pasan por alto. Por ejemplo, las pruebas fuera de l√≠nea podr√≠an detectar si la tasa de √©xito de un agente de generaci√≥n de c√≥digo en un conjunto conocido de problemas est√° mejorando, mientras que el monitoreo en l√≠nea podr√≠a alertarte de que los usuarios han comenzado a hacer una nueva categor√≠a de preguntas con las que el agente tiene dificultades. Combinar ambos proporciona una imagen m√°s robusta.

De hecho, muchos equipos adoptan un ciclo: _evaluaci√≥n fuera de l√≠nea ‚Üí implementar nueva versi√≥n del agente ‚Üí monitorear m√©tricas en l√≠nea y recopilar nuevos ejemplos de fallos ‚Üí agregar esos ejemplos al conjunto de prueba fuera de l√≠nea ‚Üí iterar_. De esta manera, la evaluaci√≥n es continua y siempre mejorando.

## üßë‚Äçüíª Veamos c√≥mo funciona esto en la pr√°ctica

En la siguiente secci√≥n, veremos ejemplos de c√≥mo podemos usar herramientas de observabilidad para monitorear y evaluar nuestro agente.
